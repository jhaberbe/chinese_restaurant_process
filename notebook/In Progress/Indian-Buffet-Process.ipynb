{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42315ecb",
   "metadata": {},
   "source": [
    "# Indian Buffet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76482a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.IndianBuffetProcess"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IndianBuffetProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95229478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src/crp\")\n",
    "from table import DirichletMultinomialTable, ChineseRestaurantTable, NegativeBinomialTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "633b0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NegativeBinomialTable(ChineseRestaurantTable):\n",
    "    \"\"\"\n",
    "    Represents a table in a Negative Binomial model.\n",
    "\n",
    "    Attributes:\n",
    "        data (np.ndarray): A 2D array storing the table's data.\n",
    "        members (set): A set of unique indices representing the table's members.\n",
    "        alpha (np.ndarray): A 1D array representing the shape parameters.\n",
    "        beta (np.ndarray): A 1D array representing the rate parameters.\n",
    "        reference_total (float): The total count of self.data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes a NegativeBinomialTable object with the given data.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): A 2D array to be stored in the table.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If the data is not a numpy array.\n",
    "        \"\"\"\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            raise TypeError(\"Data must be a numpy array\")\n",
    "        self.data = np.array(data)\n",
    "        self.members = set()\n",
    "        self.alpha = np.ones(self.data.shape[1])  # prior shape\n",
    "        self.beta = np.ones(self.data.shape[1])\n",
    "        self.reference_total = np.mean(np.sum(self.data, axis=1))\n",
    "\n",
    "    def add_member(self, index: int):\n",
    "        \"\"\"\n",
    "        Adds a member to the table at the specified index.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index at which the member is to be added.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the index is not a valid index.\n",
    "        \"\"\"\n",
    "        if index < 0:\n",
    "            raise ValueError(\"Index must be a non-negative integer\")\n",
    "        if index not in self.members:\n",
    "            self.members.add(index)\n",
    "            self.alpha += self.data[index]\n",
    "            self.beta += 1  # One new data point\n",
    "\n",
    "    def remove_member(self, index: int):\n",
    "        \"\"\"\n",
    "        Removes a member from the table at the specified index.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index at which the member is to be removed.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the index is not a valid index.\n",
    "        \"\"\"\n",
    "        if index < 0:\n",
    "            raise ValueError(\"Index must be a non-negative integer\")\n",
    "        if index in self.members:\n",
    "            self.members.remove(index)\n",
    "            self.alpha -= self.data[index]\n",
    "            self.beta -= 1\n",
    "\n",
    "    def _gamma_poisson_log_likelihood(self, count: np.ndarray, alpha: np.ndarray, beta: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log likelihood of the Negative Binomial model.\n",
    "\n",
    "        Args:\n",
    "            count (np.ndarray): A 1D array representing the count.\n",
    "            alpha (np.ndarray): A 1D array representing the shape parameters.\n",
    "            beta (np.ndarray): A 1D array representing the rate parameters.\n",
    "\n",
    "        Returns:\n",
    "            float: The log likelihood of the Negative Binomial model.\n",
    "        \"\"\"\n",
    "        count = np.asarray(count).reshape(-1)\n",
    "        alpha = np.asarray(alpha).reshape(-1)\n",
    "        beta = np.asarray(beta).reshape(-1)\n",
    "\n",
    "        # Compute size factor from total count vs. mean total count of self.data\n",
    "        total = np.sum(count)\n",
    "        reference_total = self.reference_total\n",
    "        size_factor = total / reference_total if reference_total > 0 else 1.0\n",
    "        log_sf = np.log(size_factor)\n",
    "\n",
    "        # Gamma-Poisson log-likelihood with offset\n",
    "        term1 = gammaln(count + alpha)\n",
    "        term2 = -gammaln(count + 1)\n",
    "        term3 = -gammaln(alpha)\n",
    "        term4 = alpha * np.log(beta / (beta + np.exp(log_sf)))\n",
    "        term5 = count * np.log(np.exp(log_sf) / (beta + np.exp(log_sf)))\n",
    "\n",
    "        return np.sum(term1 + term2 + term3 + term4 + term5)\n",
    "\n",
    "    def return_parameters(self, index: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the parameters of the table.\n",
    "\n",
    "        Args:\n",
    "            index (int, optional): The index at which to return the parameters. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A pandas DataFrame containing the shape and rate parameters.\n",
    "        \"\"\"\n",
    "        parameters = pd.DataFrame({\n",
    "            \"log_mean\": self.log_mean(),\n",
    "            \"dispersion\": self.dispersion()\n",
    "        })\n",
    "\n",
    "        if index is not None:\n",
    "            parameters.index = index\n",
    "            return parameters\n",
    "        else:\n",
    "            return parameters\n",
    "\n",
    "    def log_mean(self):\n",
    "        return np.log(self.alpha) - np.log(self.beta)\n",
    "\n",
    "    def dispersion(self):\n",
    "        return self.alpha  # shape parameter r\n",
    "\n",
    "    def mean(self):\n",
    "        return self.alpha / self.beta\n",
    "\n",
    "    def log_likelihood(self, index: int, posterior: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log likelihood of the table.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the table.\n",
    "            posterior (bool, optional): A flag indicating whether to calculate the posterior. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            float: The log likelihood of the table.\n",
    "        \"\"\"\n",
    "        x = self.data[index]\n",
    "        if posterior:\n",
    "            alpha = self.alpha + x\n",
    "            beta = self.beta + 1\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "            beta = self.beta\n",
    "\n",
    "        return self._gamma_poisson_log_likelihood(x, alpha, beta)\n",
    "\n",
    "    def predict(self, count: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Makes predictions using the table's data.\n",
    "\n",
    "        Args:\n",
    "            count (np.ndarray): A 1D array representing the count.\n",
    "\n",
    "        Returns:\n",
    "            float: The prediction.\n",
    "        \"\"\"\n",
    "        return self._gamma_poisson_log_likelihood(count, self.alpha, self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139e93c",
   "metadata": {},
   "source": [
    "# Load some count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "78d3b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/chinese_restaurant_process/.venv/lib/python3.13/site-packages/scanpy/preprocessing/_highly_variable_genes.py:172: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.\n",
      "  adata.uns[\"hvg\"] = {\"flavor\": flavor}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"/home/jhaberbe/Data/choroid-plexus/new_annotations.h5ad\")\n",
    "adata = adata[adata.obs[\"Cell.Subtype\"].eq(\"Macrophage\")]\n",
    "adata = adata[adata.X.sum(axis=1) > 300]\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\")\n",
    "\n",
    "counts = adata[:, adata.var.highly_variable].X.todense()\n",
    "size_factors = np.log(counts.sum(axis=1) / counts.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741ae6e",
   "metadata": {},
   "source": [
    "# Our inference machinery\n",
    "\n",
    "First, we learn the base distribution, which we'll use as our intercept. We then pull that out. We will be performing updates to classes iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b86e85",
   "metadata": {},
   "source": [
    "Now, our goal is to use these latent features to then learn the individual features. Each latent feature will be a coefficient, we then update based on class membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "2e45466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import NegativeBinomial\n",
    "\n",
    "class LatentFeature:\n",
    "    def __init__(self, X, device):\n",
    "        self.members = set()\n",
    "        self.log_mu = nn.Parameter(torch.zeros(X.shape[1], device=device))  # On GPU\n",
    "\n",
    "    def add_member(self, index):\n",
    "        self.members.add(index)\n",
    "\n",
    "    def remove_member(self, index):\n",
    "        self.members.discard(index)\n",
    "\n",
    "    def num_members(self):\n",
    "        return len(self.members)\n",
    "\n",
    "class IBP:\n",
    "    def __init__(self, X, alpha, device=None):\n",
    "        self.device = device or X.device\n",
    "        self.data = X.to(self.device)  # Ensure data is on GPU\n",
    "        self.N, self.D = X.shape\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.size_factor = (X.sum(dim=1) / X.sum(dim=1).mean()).log().to(self.device)\n",
    "\n",
    "        self.latent_features = {}\n",
    "        self.membership = {i: set() for i in range(self.N)}\n",
    "\n",
    "        self.log_mu_intercept = nn.Parameter(torch.zeros(self.D, device=self.device))\n",
    "        self.log_disp = nn.Parameter(torch.ones(self.D, device=self.device))\n",
    "\n",
    "        self.null_ll_cache = {}\n",
    "        self.optimizer = torch.optim.Adam([self.log_mu_intercept, self.log_disp], lr=0.1)\n",
    "\n",
    "    def add_class(self):\n",
    "        k = 0\n",
    "        while k in self.latent_features:\n",
    "            k += 1\n",
    "        self.latent_features[k] = LatentFeature(self.data, device=self.device)\n",
    "        return k\n",
    "\n",
    "    def compute_logits(self, sample_index):\n",
    "        logits = self.size_factor[sample_index] + self.log_mu_intercept\n",
    "        for k in self.membership[sample_index]:\n",
    "            logits += self.latent_features[k].log_mu\n",
    "        return logits\n",
    "\n",
    "    def log_likelihood(self, sample_index):\n",
    "        logits = self.compute_logits(sample_index)\n",
    "        x = self.data[sample_index]\n",
    "        theta = self.log_disp.exp()\n",
    "        logit_param = logits - (theta + logits.exp()).log()\n",
    "        nb = NegativeBinomial(total_count=theta, logits=logit_param)\n",
    "        return nb.log_prob(x).sum()\n",
    "\n",
    "    def fit_intercept(self, n_iter=100):\n",
    "        for _ in trange(n_iter):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for i in range(self.N):\n",
    "                logits = self.size_factor[i] + self.log_mu_intercept\n",
    "                theta = self.log_disp.exp()\n",
    "                x = self.data[i]\n",
    "                logit_param = logits - (theta + logits.exp()).log()\n",
    "                nb = NegativeBinomial(total_count=theta, logits=logit_param)\n",
    "                loss -= nb.log_prob(x).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def assignment(self, index):\n",
    "        x = self.data[index]\n",
    "        self.null_ll_cache[index] = self.log_likelihood(index)\n",
    "\n",
    "        # Remove index from current features\n",
    "        to_remove = list(self.membership[index])\n",
    "        for k in to_remove:\n",
    "            self.membership[index].remove(k)\n",
    "            self.latent_features[k].remove_member(index)\n",
    "\n",
    "            # Delete latent feature if empty\n",
    "            if self.latent_features[k].num_members() == 0:\n",
    "                del self.latent_features[k]\n",
    "        \n",
    "        base_ll = self.null_ll_cache[index]\n",
    "\n",
    "        # Reconsider assignment to existing features\n",
    "        existing_keys = list(self.latent_features.keys())  # avoid mutation during loop\n",
    "        for k in existing_keys:\n",
    "            self.membership[index].add(k)\n",
    "            self.latent_features[k].add_member(index)\n",
    "\n",
    "            proposed_ll = self.log_likelihood(index)\n",
    "            log_p = proposed_ll - base_ll\n",
    "            p = torch.sigmoid(log_p)\n",
    "\n",
    "            if torch.rand(1, device=self.device).item() < p:\n",
    "                base_ll = proposed_ll\n",
    "            else:\n",
    "                self.membership[index].remove(k)\n",
    "                self.latent_features[k].remove_member(index)\n",
    "\n",
    "                if self.latent_features[k].num_members() == 0:\n",
    "                    del self.latent_features[k]\n",
    "\n",
    "        # Sample new features\n",
    "        lambda_new = self.alpha / self.N\n",
    "        num_new = torch.poisson(torch.tensor(lambda_new, device=self.device)).item()\n",
    "        for _ in range(int(num_new)):\n",
    "            new_k = self.add_class()\n",
    "            self.latent_features[new_k].add_member(index)\n",
    "            self.membership[index].add(new_k)\n",
    "\n",
    "        # Optional: Update parameters locally\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = -self.log_likelihood(index)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cea6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303885/3353483539.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ibp = IBP(X=torch.tensor(X, dtype=torch.float32).to(\"cuda\"), alpha=1.0, device=\"cuda\")\n",
      " 11%|█         | 11/100 [00:06<00:52,  1.71it/s]"
     ]
    }
   ],
   "source": [
    "ibp = IBP(X=torch.tensor(X, dtype=torch.float32).to(\"cuda\"), alpha=1.0, device=\"cuda\")\n",
    "ibp.fit_intercept()\n",
    "\n",
    "for step in range(100):\n",
    "    for i in trange(X.shape[0]):\n",
    "        ibp.assignment(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fc326",
   "metadata": {},
   "source": [
    "#### Explicit formula for the value of p(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eb04f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln\n",
    "def feature_probability(feature, total = 1000):\n",
    "    structure_term = (feature.alpha / feature.n_classes)\n",
    "\n",
    "    term_1 = (np.log(structure_term) + gammaln(len(feature.members) + structure_term)) \n",
    "    term_2 = gammaln(total - len(feature.members) + 1)\n",
    "    term_3 = gammaln(total + 1 + structure_term)\n",
    "    \n",
    "    return term_1 + term_2 - term_3\n",
    "\n",
    "def total_probability(feature_dict):\n",
    "    return np.sum([\n",
    "        feature_probability(feature_dict[key]) \n",
    "        for key in feature_dict \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "74e4c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-6439.465591024762)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_probability(latent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91104da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
